\section{Erwartungswerte und Momente von Zufallsvariablen}
\paragraph{Definition: Erwartungswert auf einem diskreten Wahrscheinlichkeitsraum}
Der \textbf{Erwartungswert} einer $\R$-wertigen Zufallsvariable $X:\Omega\rightarrow\R$ auf einem \underline{diskreten} Wahrscheinlichkeitsraum $(\Omega,\PP)$ ist definiert als:
\begin{tightcenter}
	$\E_\PP[X]\coloneqq\E[X]\coloneqq\sum\limits_{x\in X(\Omega)}x\cdot \PP(X=x)=\sum\limits_{x\in X(\Omega)}x\cdot\PP^X(\{x\})$
\end{tightcenter}
falls $\sum\limits_{x\in X(\Omega)}|x|\cdot\PP(X=x)<\infty$.
$\E_\PP[X]$ heißt auch \textbf{Mittelwert} von $\PP^X$.

\paragraph{Definition: Erwartungswert für stetige Zufallsvariablen}
Der \textbf{Erwartungswert} einer stetigen Zufallsvariable $X$ mit Dichte $f_X$ wird definiert als:
\begin{tightcenter}
	$\E[X]=\int_{-\infty}^{\infty}x\cdot f_X(x)dx$
\end{tightcenter}
falls $\int_{-\infty}^{\infty}|x|\cdot f_X(x)dx<\infty$.
$\E[X]$ heißt auch \textbf{Mittelwert} von $\PP^X$.

\paragraph{Transformationssatz}
Sei $(\Omega,\AG,\PP)$ ein Wahrscheinlichkeitsraum, $X:\Omega\rightarrow S$ eine diskrete/stetige Zufallsvariable mit (Zähl-)Dichte $f_X$ und $g:S\rightarrow\R$ eine Funktion.
Die Zufallsvariable $g(X)=g\circ X$ beisitzt genau dann einen endlichen Erwartungswert bzgl. $\PP$, wenn $g$ einen endlichen Erwartungswert bzgl. $\PP^X$ besitzt.
In diesem Fall gilt:
\[   
\E_\PP[g(X)]=\E_{\PP^X}[g]=
\begin{cases}
	\sum\limits_{x\in S}g(x)\cdot f_X(x) & \qquad\text{falls $X$ diskret}\\
	\int_{-\infty}^{\infty}g(x)\cdot f_X(x)dx & \qquad\text{falls $X$ stetig}
\end{cases}
\]
Der Transformationssatz gilt auch für Zufallsvariablen $X,Y$ mit gemeinsamer Zähldichte $f_{X,Y}$, d.h.
\[   
\E[g(X,Y)]=
\begin{cases}
	\sum\limits_{x\in X(\Omega)}\sum\limits_{y\in Y(\Omega}g(x,y)\cdot f_{X,Y}(x,y) & \qquad\text{falls $X,Y$ diskret}\\
	\int_{-\infty}^{\infty}(\int_{-\infty}^{\infty}g(x,y)\cdot f_{X,Y}(x,y)dy)dx & \qquad\text{falls $X$ stetig}
\end{cases}
\]

\paragraph{Rechenregeln für Erwartungswerte}
Seien $X,Y$ diskrete/stetige Zufallsvariablen auf einem Wahrscheinlichkeitsraum $(\Omega,\AG,\PP)$, die Erwartungswerte besitzen.
Dann gilt:
\begin{itemize}
	\item $\E[aX+Y]=a\cdot\E[X]+\E[Y]$ für alle $a\in\R$ (\textbf{Linearität})
	\item Gilt $X\leq Y$, dann folgt $\E[X]\leq\E[Y]$ (\textbf{Monotonie})
	\item Wenn $f_X$ symmetrisch zu $x=a$ ist, dann gilt $\E[X]=a$
\end{itemize}

\paragraph{Siebformel von Sylvester-Poincar$\boldsymbol{\acute{e}}$}
Seien $A_1,\ldots,A_n\in\AG$ Ereignisse im einem diskreten Wahrscheinlichkeitsraum $(\Omega,\AG,\PP)$.
Dann gilt:
\begin{tightcenter}
	$\PP(\bigcup\limits_{i=1}^{n}A_i)=\sum\limits_{I\subseteq\{1,\ldots,n\}, I\neq\emptyset}(-1)^{|I|+1}\cdot\PP(\bigcap\limits_{i\in I}A_i)$
\end{tightcenter}

\paragraph{Darstellungsformel für nicht-negative Zufallsvariablen}
Ist $X$ eine $\N_0$-wertige oder $\R_+$-wertige Zufallsvariable, so gilt
\[   
\E[X]=
\begin{cases}
	\sum\limits_{n=1}^{\infty}\PP(X\geq n) & \qquad\text{falls $X$ diskret}\\
	\int_{0}^{\infty}\PP(X\geq x) & \qquad\text{falls $X$ stetig}
\end{cases}
\]

\paragraph{Multiplikationsformel für Erwartungswerte}
Sind $X$ und $Y$ stochastisch unabhängige reellwertige Zufallsvariablen mit Erwartungswerten $\E[X]$ und $\E[Y]$, so gilt
\begin{tightcenter}
	$\E[X\cdot Y]=\E[X]\cdot\E[Y]$
\end{tightcenter}
Die Umkehrung gilt im Allgemeinen nicht!

\paragraph{Definition: Variation, Standardabweichung und Momente}
\begin{itemize}
	\item Existieren $\E[X]$ und $\E[X^2]$ so ist die \textbf{Varianz} von $X$ durch
	\[   
	Var(X)= \E[(X-\E[X])^2]=
	\begin{cases}
		\sum\limits_{x\in X(\Omega)}(x-\E[X])^2f_X(x) & \qquad\text{falls $X$ diskret}\\
		\int_{-\infty}^{\infty}(x-\E[X])^2f_X(x)dx & \qquad\text{falls $X$ stetig}
	\end{cases}
	\]
	definiert.
	$\sqrt{Var(X)}$ heißt \textbf{Standardabweichung} von $X$.
	\item Für $k\in\N$ heißt $\E[X^k]$ das $\boldsymbol{k}$\textbf{-te Moment von} $X$.
	Dabei ist $X^k:\Omega\rightarrow\R$ definiert durch $X^k(\omega)=(X(\omega))^k$
\end{itemize}

\newpage
\paragraph{Eigenschaften der Varianz}
\begin{itemize}
	\item $Var(a\cdot X+b)=a^2\cdot Var(X)$ für alle $a,b\in\R$. Die Varianz ist nicht linear!
	\item $Var(X)=\E[X^2]-\E[X]^2$, also $\E[X]^2\leq \E[X^2]$, da $0\leq Var(X)$
	\item $\E[(X-a)^2]=Var(X)+(\E[X]-a)^2$ für alle $a\in\R$
	\item Die Minimalstelle der Funktion $a\mapsto\E[(X-a)^2]$ ist $a=\E[X]$
\end{itemize}

\paragraph{Wichtige Beispiele}
\begin{itemize}
	\item Wenn $X\sim Ber_p$ mit $p\in [0,1]$, dann gilt $\E[X]=p$ und $Var(X)=p(1-p)$
	\item Wenn $X\sim Poiss_\lambda$, dann gilt $\E[X^2]=\lambda^2+\lambda$ und $Var(X)=\lambda$
	\item Wenn $X\sim U([a,b])$ gleichverteilt, dann gilt $\E[X^2]=\frac{b^2+ab+a^2}{3}$ und $Var(X)=\frac{(b-a)^2}{12}$
	\item Wenn $Y\sim N_{(0,1)}$, dann gilt $\E[Y]=0$ und $Var(Y)=1$ sowie wegen\\ $X\coloneqq\sigma Y+\mu\sim N_{(0,1)}$ folgt $Var(X)=\sigma^2$
\end{itemize}

\paragraph{Markov- und Chebyshev-Ungleichung}
\begin{itemize}
	\item \textbf{Markov-Ungleichung:} $\PP(|X|\geq c)\leq\cfrac{\E[|X|]}{c}$ \qquad$\forall c>0$
	\item \textbf{Chebyshev-Ungleichung:} $\PP(|X-\E[X]|\geq c)\leq\cfrac{Var(X)}{c^2}$ \qquad$\forall c>0$
\end{itemize}

\paragraph{Definition: Kovarianz und Korrelation}
Seien $X$ und $Y$ Zufallsvariablen.
\begin{tightcenter}
	$Cov(X,Y)=\E[(X-\E[X])\cdot(Y-\E[Y])]=\E[XY]-\E[X]\cdot\E[Y]$
\end{tightcenter}
heißt die \textbf{Kovarianz} von $X$ und $Y$.
\begin{tightcenter}
	$Corr(X,Y)=\cfrac{Cov(X,Y)}{\sqrt{Var(X)\cdot Var(Y)}}$
\end{tightcenter}
heißt \textbf{Korrelation} von $X$ und $Y$, falls $Var(X)>0$ und $Var(Y)>0$.
$X$ und $Y$ heißen \textbf{unkorreliert}, falls $Cov(X,Y)=0$.\\
Die Kovarianz ist bilinear, d.h. $Cov(aX+b,cY+d)=ac\cdot Cov(X,Y)$.\\
Ist $Z$ eine weitere Zufallsvariable, so gilt $Cov(X,Y+Z)=Cov(X,Y)+Cov(X,Z)$.
Jede unabhängige Zufallsvariable ist unkorreliert, umgekehrt aber nicht!

\paragraph{Varianz von Summen von Zufallsvariablen}
Für Zufallsvariablen $X_1\cdot X_n$ gilt:
\begin{tightcenter}
	$Var(\sum\limits_{i=1}^{n}X_i)=\sum\limits_{i=1}^{n}Var(X_i)+2\sum\limits_{1\leq i<j\leq n}Cov(X_i,X_j)$
\end{tightcenter}
Sind die Variablen unabhängig (oder schwächer: unkorreliert), so gilt insbesondere:
\begin{tightcenter} 
	$Var(\sum\limits_{i=1}^{n}X_i)=\sum\limits_{i=1}^{n}Var(X_i)$
\end{tightcenter}

\paragraph{Definition: Median einer Zufallsvariable}
Neben dem Erwartungswert gibt es eitere Parameter, die \enquote{mittlere Werte} einer Zufallsvariable beschreiben.
Eine Zahl $m(X)$ heißt \textbf{Median} von $X$ bzw. $\PP^X$, falls gilt:
\begin{tightcenter}
	$\PP(X\leq m(X))\geq\frac{1}{2}$ und $\PP(X\geq m(X))\geq\frac{1}{2}$
\end{tightcenter}
Mediane sind im Allgemeinen nicht eindeutig. 
Der Median ist ein \textbf{Lageparameter}, d.h. es gilt: $m$ ist Median von $X\iff am+b$ ist Median von $aX+b$.

\paragraph{Definition: Quantil}
Ein Quantil ist eine Verallgemeinerung des Medians.
Für $X$ mit Verteilungsfunktion $F_X$ und $0<p<1$ heißt
\begin{tightcenter}
	$t_p\coloneqq t_p(X)\coloneqq F_{X}^{-1}(p)\coloneqq inf\{x\in\R:F_X(x)\geq p\}$
\end{tightcenter}
$\boldsymbol{p}$\textbf{-Quantil} von $F_X$ bzw. $X$. 
$t_{1/2}$ heißt Median, $t_{1/4}$ \textbf{unteres Quartil} und $t_{3/4}$ \textbf{oberes Quartil}.